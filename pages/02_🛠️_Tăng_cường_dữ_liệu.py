import streamlit as st
from utils.helpers import display_title_icon, process_uploaded_file
import nlpaug.augmenter.word as naw
from transformers import MarianMTModel, MarianTokenizer
import pandas as pd

display_title_icon("ğŸ› ï¸", "TÄƒng cÆ°á»ng Dá»¯ liá»‡u VÄƒn báº£n")

# Lá»›p xá»­ lÃ½ tÄƒng cÆ°á»ng dá»¯ liá»‡u
class DataAugmentation:
    def __init__(self):
        """Khá»Ÿi táº¡o cÃ¡c bá»™ tÄƒng cÆ°á»ng dá»¯ liá»‡u NLP"""
        self.synonym_aug = naw.SynonymAug(aug_src="wordnet")  # Thay tá»« Ä‘á»“ng nghÄ©a
        self.swap_aug = naw.RandomWordAug(action="swap")  # Äáº£o vá»‹ trÃ­ tá»«
        self.delete_aug = naw.RandomWordAug(action="delete")  # XÃ³a tá»«
        self.insert_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action="insert", aug_p=0.3)  # ThÃªm tá»«
        
        # Khá»Ÿi táº¡o mÃ´ hÃ¬nh dá»‹ch mÃ¡y
        self.translator_en_de = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de")
        self.tokenizer_en_de = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
        self.translator_de_en = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-de-en")
        self.tokenizer_de_en = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-de-en")
        
    def synonym_augmentation(self, text):
        """Thay tháº¿ tá»« Ä‘á»“ng nghÄ©a"""
        return self.synonym_aug.augment(text)[0]

    def swap_words(self, text):
        """HoÃ¡n Ä‘á»•i vá»‹ trÃ­ tá»«"""
        return self.swap_aug.augment(text)[0]

    def delete_words(self, text):
        """XÃ³a tá»« ngáº«u nhiÃªn"""
        return self.delete_aug.augment(text)[0]

    def insert_words(self, text):
        """ThÃªm tá»« ngáº«u nhiÃªn vÃ o vÄƒn báº£n"""
        return self.insert_aug.augment(text)[0]

    def back_translation(self, text):
        """Dá»‹ch ngÆ°á»£c báº±ng mÃ´ hÃ¬nh Helsinki-NLP"""
        if isinstance(text, str):  # Äáº£m báº£o text lÃ  danh sÃ¡ch
            text = [text]

        tokens = self.tokenizer_en_de(text, return_tensors="pt", padding=True, truncation=True)
        translated = self.translator_en_de.generate(**tokens)
        translated_text = self.tokenizer_en_de.batch_decode(translated, skip_special_tokens=True)

        tokens = self.tokenizer_de_en(translated_text, return_tensors="pt", padding=True, truncation=True)
        back_translated = self.translator_de_en.generate(**tokens)
        back_translated_text = self.tokenizer_de_en.batch_decode(back_translated, skip_special_tokens=True)

        return back_translated_text[0] if len(back_translated_text) > 0 else text[0]

# Khá»Ÿi táº¡o augmenter
if 'augmenter' not in st.session_state:
    st.session_state.augmenter = DataAugmentation()

# Giao diá»‡n Streamlit
st.markdown("""
## CÃ¡c ká»¹ thuáº­t tÄƒng cÆ°á»ng dá»¯ liá»‡u vÄƒn báº£n

Chá»n cÃ¡c phÆ°Æ¡ng phÃ¡p báº¡n muá»‘n Ã¡p dá»¥ng cho vÄƒn báº£n:
""")

# Pháº§n nháº­p liá»‡u
st.header("1. Nháº­p liá»‡u")
input_method = st.radio("Chá»n cÃ¡ch nháº­p liá»‡u:", ["Nháº­p trá»±c tiáº¿p", "Táº£i lÃªn file vÄƒn báº£n"])

text_input = ""
if input_method == "Nháº­p trá»±c tiáº¿p":
    text_input = st.text_area("Nháº­p vÄƒn báº£n cáº§n tÄƒng cÆ°á»ng:", 
                            "This paper will use DLmethods to improve the performance of sentiment analysis.")
else:
    uploaded_file = st.file_uploader("Táº£i lÃªn file (.txt, .csv, .docx, .pdf, .json)", type=["txt", "csv", "docx", "pdf", "json"])
    if uploaded_file:
        texts = process_uploaded_file(uploaded_file)
        if texts:
            text_input = "\n".join(texts)
            st.text_area("Ná»™i dung file:", text_input, height=150)

# Pháº§n lá»±a chá»n phÆ°Æ¡ng phÃ¡p
st.header("2. Lá»±a chá»n phÆ°Æ¡ng phÃ¡p")
col1, col2 = st.columns(2)

with col1:
    use_synonym = st.checkbox("Thay tháº¿ tá»« Ä‘á»“ng nghÄ©a", value=True)
    use_swap = st.checkbox("Äáº£o vá»‹ trÃ­ tá»«", value=True)
    use_insert = st.checkbox("ThÃªm tá»« ngáº«u nhiÃªn")

with col2:
    use_delete = st.checkbox("XÃ³a tá»« ngáº«u nhiÃªn")
    use_backtrans = st.checkbox("Dá»‹ch qua láº¡i (back translation)")

# Pháº§n thá»±c thi
if st.button("ğŸ¯ Thá»±c thi") and text_input:
    results = []
    original_text = text_input
    
    # Táº¡o báº£n gá»‘c
    results.append({
        "PhÆ°Æ¡ng phÃ¡p": "VÄƒn báº£n gá»‘c",
        "Káº¿t quáº£": original_text,
        "Äá»™ dÃ i": len(original_text.split())
    })
    
    # Ãp dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c chá»n
    if use_synonym:
        augmented = st.session_state.augmenter.synonym_augmentation(original_text)
        results.append({
            "PhÆ°Æ¡ng phÃ¡p": "Thay tá»« Ä‘á»“ng nghÄ©a",
            "Káº¿t quáº£": augmented,
            "Äá»™ dÃ i": len(augmented.split())
        })
    
    if use_swap:
        augmented = st.session_state.augmenter.swap_words(original_text)
        results.append({
            "PhÆ°Æ¡ng phÃ¡p": "Äáº£o vá»‹ trÃ­ tá»«",
            "Káº¿t quáº£": augmented,
            "Äá»™ dÃ i": len(augmented.split())
        })
    
    if use_insert:
        augmented = st.session_state.augmenter.insert_words(original_text)
        results.append({
            "PhÆ°Æ¡ng phÃ¡p": "ThÃªm tá»« ngáº«u nhiÃªn",
            "Káº¿t quáº£": augmented,
            "Äá»™ dÃ i": len(augmented.split())
        })
    
    if use_delete:
        augmented = st.session_state.augmenter.delete_words(original_text)
        results.append({
            "PhÆ°Æ¡ng phÃ¡p": "XÃ³a tá»« ngáº«u nhiÃªn",
            "Káº¿t quáº£": augmented,
            "Äá»™ dÃ i": len(augmented.split())
        })
    
    if use_backtrans:
        augmented = st.session_state.augmenter.back_translation(original_text)
        results.append({
            "PhÆ°Æ¡ng phÃ¡p": "Dá»‹ch qua láº¡i",
            "Káº¿t quáº£": augmented,
            "Äá»™ dÃ i": len(augmented.split())
        })
    
    # Hiá»ƒn thá»‹ káº¿t quáº£
    st.header("3. Káº¿t quáº£ TÄƒng cÆ°á»ng")
    df_results = pd.DataFrame(results)
    st.dataframe(df_results, use_container_width=True)
    
    # Táº£i káº¿t quáº£ vá»
    csv = df_results.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ Táº£i káº¿t quáº£ vá» (CSV)",
        data=csv,
        file_name='augmented_results.csv',
        mime='text/csv'
    )
    
    # Hiá»ƒn thá»‹ chi tiáº¿t tá»«ng káº¿t quáº£
    st.header("Chi tiáº¿t káº¿t quáº£")
    for result in results:
        with st.expander(f"ğŸ” {result['PhÆ°Æ¡ng phÃ¡p']}"):
            st.write(result["Káº¿t quáº£"])
            st.caption(f"Äá»™ dÃ i: {result['Äá»™ dÃ i']} tá»«")

elif not text_input:
    st.warning("Vui lÃ²ng nháº­p hoáº·c táº£i lÃªn vÄƒn báº£n trÆ°á»›c khi thá»±c thi")