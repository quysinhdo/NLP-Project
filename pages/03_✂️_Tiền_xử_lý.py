import streamlit as st
import pandas as pd
import spacy
import nltk
import contractions
import difflib
from nltk.stem import PorterStemmer
from spellchecker import SpellChecker
from spacy import displacy
from utils.helpers import display_title_icon, process_uploaded_file

display_title_icon("‚úÇÔ∏è", "Ti·ªÅn X·ª≠ L√Ω VƒÉn B·∫£n")

# T·∫£i c√°c t√†i nguy√™n c·∫ßn thi·∫øt cho ti·∫øng Anh
nltk.download('punkt')
nltk.download('wordnet')

# T·∫£i m√¥ h√¨nh ng√¥n ng·ªØ ti·∫øng Anh
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    st.error("Vui l√≤ng c√†i ƒë·∫∑t m√¥ h√¨nh ng√¥n ng·ªØ ti·∫øng Anh tr∆∞·ªõc:")
    st.code("python -m spacy download en_core_web_sm")
    st.stop()

# Kh·ªüi t·∫°o c√°c c√¥ng c·ª• x·ª≠ l√Ω cho ti·∫øng Anh
stemmer = PorterStemmer()
spell = SpellChecker(language='en')  # B·ªô ki·ªÉm tra ch√≠nh t·∫£ ti·∫øng Anh

# =============================================
# PH·∫¶N 1: L·ªöP X·ª¨ L√ù TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N TI·∫æNG ANH
# =============================================

class EnglishTextPreprocessor:
    def __init__(self, text):
        self.text = text
        self.original_words_with_spaces = self._tokenize_with_spaces(text)
        self.doc = nlp(text)

    def _tokenize_with_spaces(self, text_input):
        """T√°ch vƒÉn b·∫£n th√†nh t·ª´ v√† kho·∫£ng tr·∫Øng xen k·∫Ω."""
        import re        
        return re.findall(r"[\w'-]+|\s+", text_input)

    def _reconstruct_text(self, tokens_with_spaces):
        """Gh√©p l·∫°i c√°c token v√† kho·∫£ng tr·∫Øng."""
        return "".join(tokens_with_spaces)
    
    def sentence_tokenization(self):
        """T√°ch c√¢u"""
        return [sent.text for sent in self.doc.sents]
    
    def word_tokenization(self):
        """T√°ch t·ª´"""
        return [token.text for token in self.doc]
    
    def remove_stopwords(self):
        """X√≥a stopwords v√† tr·∫£ v·ªÅ c√¢u ho√†n ch·ªânh"""
        filtered_tokens = [token.text for token in self.doc if not token.is_stop and not token.is_punct]
        return " ".join(filtered_tokens)
    
    def to_lowercase(self):
        """Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng"""
        return self.text.lower()
    
    def stemming(self):
        """Stemming (Porter)"""
        return [stemmer.stem(token.text) for token in self.doc]
    
    def lemmatization(self):
        """Lemmatization"""
        return [(token.text, token.lemma_) for token in self.doc]
    
    def pos_tagging(self):
        """POS Tagging"""
        return [(token.text, token.pos_) for token in self.doc]
    
    def expand_contractions(self):
        """S·ª≠a t·ª´ vi·∫øt t·∫Øt v√† highlight c√°c t·ª´ ƒë∆∞·ª£c s·ª≠a."""
        original_text_reconstructed = self._reconstruct_text(self.original_words_with_spaces)
        fixed_text = contractions.fix(original_text_reconstructed)        
        fixed_words_with_spaces = self._tokenize_with_spaces(fixed_text)        
        s = difflib.SequenceMatcher(None, self.original_words_with_spaces, fixed_words_with_spaces)        
        highlighted_parts = []
        original_parts_for_comparison = []

        for tag, i1, i2, j1, j2 in s.get_opcodes():
            original_segment = self.original_words_with_spaces[i1:i2]
            fixed_segment = fixed_words_with_spaces[j1:j2]
            if tag == 'replace':    
                original_joined = "".join(original_segment).strip()
                fixed_joined = "".join(fixed_segment).strip()
                if original_joined != fixed_joined:
                    highlighted_parts.append(f"<mark>{self._reconstruct_text(fixed_segment)}</mark>")
                else:
                    highlighted_parts.append(self._reconstruct_text(fixed_segment))
                original_parts_for_comparison.append(self._reconstruct_text(original_segment))
            elif tag == 'delete':                
                original_parts_for_comparison.append(self._reconstruct_text(original_segment))
                pass
            elif tag == 'insert':
                highlighted_parts.append(f"<mark>{self._reconstruct_text(fixed_segment)}</mark>")                
            elif tag == 'equal':
                highlighted_parts.append(self._reconstruct_text(fixed_segment))
                original_parts_for_comparison.append(self._reconstruct_text(original_segment))        
        highlighted_html = "".join(highlighted_parts)        
        if original_text_reconstructed.lower() == fixed_text.lower() and original_text_reconstructed == fixed_text:
            is_truly_changed = any(tag != 'equal' for tag, _, _, _, _ in s.get_opcodes())
            if not is_truly_changed:
                 highlighted_html = fixed_text

        return highlighted_html, fixed_text
    
    def correct_spelling(self):
        """S·ª≠a l·ªói ch√≠nh t·∫£ v√† highlight c√°c t·ª´ ƒë∆∞·ª£c s·ª≠a."""        
        corrected_text_parts = []
        highlighted_html_parts = []        
        words_only = [token for token in self.original_words_with_spaces if token.strip() and not token.isspace()]
        misspelled = spell.unknown(words_only)
        current_word_index = 0
        for part in self.original_words_with_spaces:
            if part.strip() and not part.isspace():
                original_word = part                
                if original_word in misspelled:
                    corrected_word = spell.correction(original_word)
                    if corrected_word and corrected_word != original_word:
                        highlighted_html_parts.append(f"<mark>{corrected_word}</mark>")
                        corrected_text_parts.append(corrected_word)
                    else:
                        highlighted_html_parts.append(original_word)
                        corrected_text_parts.append(original_word)
                else:
                    highlighted_html_parts.append(original_word)
                    corrected_text_parts.append(original_word)
                current_word_index += 1
            else:
                highlighted_html_parts.append(part)
                corrected_text_parts.append(part)
                
        highlighted_html = "".join(highlighted_html_parts)
        corrected_text = "".join(corrected_text_parts)
        
        return highlighted_html, corrected_text
    
    def named_entity_recognition(self):
        """Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)"""
        return [(ent.text, ent.label_) for ent in self.doc.ents]
    
    def visualize_entities(self):
        """Hi·ªÉn th·ªã NER tr·ª±c quan"""
        html = displacy.render(self.doc, style="ent", page=True)
        return html

# =============================================
# PH·∫¶N 2: GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (TI·∫æNG VI·ªÜT)
# =============================================

st.markdown("""
## C√°c ch·ª©c nƒÉng ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n

Ch·ªçn ph∆∞∆°ng ph√°p x·ª≠ l√Ω v√† nh·∫≠p vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω:
""")

# Ph·∫ßn nh·∫≠p li·ªáu
input_method = st.radio("Ch·ªçn c√°ch nh·∫≠p li·ªáu:", 
                       ["Nh·∫≠p tr·ª±c ti·∫øp", "T·∫£i l√™n file vƒÉn b·∫£n"], 
                       horizontal=True)

text_input = ""
if input_method == "Nh·∫≠p tr·ª±c ti·∫øp":
    text_input = st.text_area("Nh·∫≠p vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω:", 
                            "Natural language processing (NLP) is a subfield of artificial intelligence.")
else:
    uploaded_file = st.file_uploader("T·∫£i l√™n file (.txt, .csv, .docx, .pdf, .json)", type=["txt", "csv", "docx", "pdf", "json"])
    if uploaded_file:
        texts = process_uploaded_file(uploaded_file)
        if texts:
            text_input = "\n".join(texts)
            st.text_area("N·ªôi dung file:", text_input, height=150)

# L·ª±a ch·ªçn ph∆∞∆°ng ph√°p x·ª≠ l√Ω
st.subheader("L·ª±a ch·ªçn ph∆∞∆°ng ph√°p ti·ªÅn x·ª≠ l√Ω")
preprocess_method = st.selectbox(
    "Ch·ªçn ch·ª©c nƒÉng x·ª≠ l√Ω:",
    options=[
        "T√°ch c√¢u",
        "T√°ch t·ª´", 
        "X√≥a stopwords",
        "Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng",
        "Stemming (Porter)",
        "Lemmatization",
        "POS Tagging",
        "S·ª≠a t·ª´ vi·∫øt t·∫Øt",
        "S·ª≠a l·ªói ch√≠nh t·∫£",
        "Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)"
    ],
    index=0
)

# N√∫t th·ª±c thi
if st.button("‚ö° Th·ª±c Hi·ªán Ti·ªÅn X·ª≠ L√Ω") and text_input:
    processor = EnglishTextPreprocessor(text_input)
    result = None
    
    with st.spinner("ƒêang x·ª≠ l√Ω vƒÉn b·∫£n..."):
        if preprocess_method == "T√°ch c√¢u":
            result = processor.sentence_tokenization()
        elif preprocess_method == "T√°ch t·ª´":
            result = processor.word_tokenization()
        elif preprocess_method == "X√≥a stopwords":
            result = processor.remove_stopwords()
        elif preprocess_method == "Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng":
            result = processor.to_lowercase()
        elif preprocess_method == "Stemming (Porter)":
            result = processor.stemming()
        elif preprocess_method == "Lemmatization":
            result = processor.lemmatization()
        elif preprocess_method == "POS Tagging":
            result = processor.pos_tagging()
        elif preprocess_method == "S·ª≠a t·ª´ vi·∫øt t·∫Øt":
            result = processor.expand_contractions()
        elif preprocess_method == "S·ª≠a l·ªói ch√≠nh t·∫£":
            result = processor.correct_spelling()
        elif preprocess_method == "Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)":
            result = processor.named_entity_recognition()
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    st.subheader("K·∫øt qu·∫£ ti·ªÅn x·ª≠ l√Ω")
    
    if preprocess_method == "Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)":
        st.markdown(processor.visualize_entities(), unsafe_allow_html=True)
        st.write("Danh s√°ch th·ª±c th·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c:")
        df = pd.DataFrame(result, columns=["Th·ª±c th·ªÉ", "Lo·∫°i"])
        st.dataframe(df, use_container_width=True)
    elif preprocess_method in ["S·ª≠a t·ª´ vi·∫øt t·∫Øt", "S·ª≠a l·ªói ch√≠nh t·∫£"]:
        highlighted_text, plain_text = result
        st.write("VƒÉn b·∫£n g·ªëc:", processor.text)
        st.write("VƒÉn b·∫£n ƒë√£ s·ª≠a:")
        st.markdown(highlighted_text, unsafe_allow_html=True)
    elif isinstance(result, list):
        if all(isinstance(item, tuple) for item in result):
            df = pd.DataFrame(result, columns=["T·ª´ g·ªëc", "K·∫øt qu·∫£"])
            st.dataframe(df, use_container_width=True)
        else:
            st.write(" ".join(result) if preprocess_method == "T√°ch t·ª´" else result)
    else:
        st.write(result)
    # st.subheader("K·∫øt qu·∫£ ti·ªÅn x·ª≠ l√Ω")
    
    # if preprocess_method == "Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)":
    #     st.markdown(processor.visualize_entities(), unsafe_allow_html=True)
    #     st.write("Danh s√°ch th·ª±c th·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c:")
    #     df = pd.DataFrame(result, columns=["Th·ª±c th·ªÉ", "Lo·∫°i"])
    #     st.dataframe(df, use_container_width=True)
    # elif isinstance(result, list):
    #     if all(isinstance(item, tuple) for item in result):
    #         df = pd.DataFrame(result, columns=["T·ª´ g·ªëc", "K·∫øt qu·∫£"])
    #         st.dataframe(df, use_container_width=True)
    #     else:
    #         st.write(result)
    # else:
    #     st.write(result)
    
    # Hi·ªÉn th·ªã th√¥ng tin th·ªëng k√™
    with st.expander("üìä Th·ªëng k√™ vƒÉn b·∫£n"):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("S·ªë c√¢u", len(processor.sentence_tokenization()))
        with col2:
            st.metric("S·ªë t·ª´", len(processor.word_tokenization()))
        with col3:
            st.metric("S·ªë t·ª´ (kh√¥ng stopwords)", len(processor.remove_stopwords()))

elif not text_input:
    st.warning("Vui l√≤ng nh·∫≠p ho·∫∑c t·∫£i l√™n vƒÉn b·∫£n ti·∫øng Anh tr∆∞·ªõc khi th·ª±c thi")

# # Gi·∫£i th√≠ch c√°c ph∆∞∆°ng ph√°p
# st.markdown("---")
# st.subheader("üìö Gi·∫£i th√≠ch ph∆∞∆°ng ph√°p")
# methods_info = {
#     "T√°ch c√¢u": "Chia vƒÉn b·∫£n ti·∫øng Anh th√†nh c√°c c√¢u ri√™ng bi·ªát",
#     "T√°ch t·ª´": "Chia c√¢u ti·∫øng Anh th√†nh c√°c t·ª´/token ri√™ng bi·ªát",
#     "X√≥a stopwords": "Lo·∫°i b·ªè c√°c t·ª´ ph·ªï bi·∫øn √≠t mang √Ω nghƒ©a trong ti·∫øng Anh",
#     "Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng": "Chu·∫©n h√≥a vƒÉn b·∫£n v·ªÅ d·∫°ng ch·ªØ th∆∞·ªùng",
#     "Stemming (Porter)": "R√∫t g·ªçn t·ª´ ti·∫øng Anh v·ªÅ d·∫°ng g·ªëc (stem)",
#     "Lemmatization": "ƒê∆∞a t·ª´ ti·∫øng Anh v·ªÅ d·∫°ng t·ª´ ƒëi·ªÉn (lemma)",
#     "POS Tagging": "G√°n nh√£n t·ª´ lo·∫°i cho t·ª´ng t·ª´ ti·∫øng Anh",
#     "S·ª≠a t·ª´ vi·∫øt t·∫Øt": "Khai tri·ªÉn c√°c t·ª´ vi·∫øt t·∫Øt ti·∫øng Anh th√†nh ƒë·∫ßy ƒë·ªß",
#     "S·ª≠a l·ªói ch√≠nh t·∫£": "S·ª≠a c√°c t·ª´ ti·∫øng Anh vi·∫øt sai ch√≠nh t·∫£",
#     "Nh·∫≠n di·ªán th·ª±c th·ªÉ (NER)": "Nh·∫≠n di·ªán t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c... trong ti·∫øng Anh"
# }

# selected_method_info = methods_info.get(preprocess_method, "")
# if selected_method_info:
#     st.info(f"**{preprocess_method}**: {selected_method_info}")